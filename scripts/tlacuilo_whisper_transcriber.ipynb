{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watching directory: ../audio_files_drop\n",
      "Processing file: /Volumes/@AEE/transcriber/audio_files_drop/08_ELEVEUR05_1124.MP3\n",
      "Original sample rate: 44100, channels: 2\n",
      "Processed sample rate: 16000, channels: 1\n",
      "Created status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 08_ELEVEUR05_1124.MP3 - estimated run time 46 mins.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to: ../transcripts/08_ELEVEUR05_1124.MP3.txt\n",
      "Deleted status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 08_ELEVEUR05_1124.MP3 - estimated run time 46 mins.txt\n",
      "Processing file: /Volumes/@AEE/transcriber/audio_files_drop/08_ELEVEUR05_1124.wav\n",
      "Error processing audio file /Volumes/@AEE/transcriber/audio_files_drop/08_ELEVEUR05_1124.wav: [Errno 2] No such file or directory: '/Volumes/@AEE/transcriber/audio_files_drop/08_ELEVEUR05_1124.wav'\n",
      "Processing file: /Volumes/@AEE/transcriber/audio_files_drop/09_CHASSEUR02_1124.MP3\n",
      "Original sample rate: 44100, channels: 2\n",
      "Processed sample rate: 16000, channels: 1\n",
      "Created status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 09_CHASSEUR02_1124.MP3 - estimated run time 61 mins.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to: ../transcripts/09_CHASSEUR02_1124.MP3.txt\n",
      "Deleted status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 09_CHASSEUR02_1124.MP3 - estimated run time 61 mins.txt\n",
      "Processing file: /Volumes/@AEE/transcriber/audio_files_drop/10_ELEVEUR06_1124.MP3\n",
      "Original sample rate: 44100, channels: 2\n",
      "Processed sample rate: 16000, channels: 1\n",
      "Created status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 10_ELEVEUR06_1124.MP3 - estimated run time 31 mins.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to: ../transcripts/10_ELEVEUR06_1124.MP3.txt\n",
      "Deleted status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 10_ELEVEUR06_1124.MP3 - estimated run time 31 mins.txt\n",
      "Processing file: /Volumes/@AEE/transcriber/audio_files_drop/11_ELEVEUR07_1124.MP3\n",
      "Original sample rate: 44100, channels: 2\n",
      "Processed sample rate: 16000, channels: 1\n",
      "Created status file: /Volumes/@AEE/transcriber/audio_files_drop/transcription started for 11_ELEVEUR07_1124.MP3 - estimated run time 91 mins.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "import pandas as pd\n",
    "import re\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "# Load error corrections from an Excel file\n",
    "def charger_corrections(chemin_fichier):\n",
    "    df = pd.read_excel(chemin_fichier)\n",
    "    corrections = dict(zip(df['erreur'], df['correction']))\n",
    "    return corrections\n",
    "\n",
    "# Replace errors in text with corrections (case-insensitive)\n",
    "def nettoyer_texte(texte, corrections):\n",
    "    for erreur, correction in corrections.items():\n",
    "        texte = re.sub(re.escape(erreur), correction, texte, flags=re.IGNORECASE)\n",
    "    return texte\n",
    "\n",
    "# Ensure the audio is in the required format (16 kHz, mono, WAV)\n",
    "def preprocess_audio(file_path):\n",
    "    try:\n",
    "        AudioSegment.ffmpeg = \"/opt/homebrew/bin/ffmpeg\" # remove as needed\n",
    "        AudioSegment.ffprobe = \"/opt/homebrew/bin/ffprobe\" # remove as needed\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        print(f\"Original sample rate: {audio.frame_rate}, channels: {audio.channels}\")\n",
    "\n",
    "        # Convert to mono and resample to 16 kHz\n",
    "        audio = audio.set_channels(1)\n",
    "        audio = audio.set_frame_rate(16000)\n",
    "        print(f\"Processed sample rate: {audio.frame_rate}, channels: {audio.channels}\")\n",
    "\n",
    "        # Export to WAV if not already in the correct format\n",
    "        if not file_path.lower().endswith('.wav'):\n",
    "            output_audio_file = file_path[:-3] + \"wav\"\n",
    "            audio.export(output_audio_file, format=\"wav\")\n",
    "            return output_audio_file, True\n",
    "        return file_path, False\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio file {file_path}: {e}\")\n",
    "        return None, False\n",
    "\n",
    "class AudioFileHandler(FileSystemEventHandler):\n",
    "    def __init__(self, pipeline, corrections, output_dir, processed_dir):\n",
    "        self.pipeline = pipeline\n",
    "        self.corrections = corrections\n",
    "        self.output_dir = output_dir\n",
    "        self.processed_dir = processed_dir\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if not event.is_directory:\n",
    "            self.process_file(event.src_path)\n",
    "\n",
    "    def process_file(self, file_path):\n",
    "        try:\n",
    "            if file_path.lower().endswith(('.wav', '.mp3', '.m4a')):\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "\n",
    "                # Preprocess the audio file\n",
    "                preprocessed_file, converted = preprocess_audio(file_path)\n",
    "                if preprocessed_file is None:\n",
    "                    return\n",
    "                # Get the audio length for status estimation\n",
    "                audio_length = AudioSegment.from_file(preprocessed_file).duration_seconds\n",
    "                estimated_time = int(audio_length / 60)\n",
    "\n",
    "                # Generate a sanitized name for status file\n",
    "                sanitized_basename = re.sub(r'[^\\w\\-_\\. ]', '_', os.path.basename(file_path))\n",
    "                status_file = os.path.join(\n",
    "                    os.path.dirname(file_path),\n",
    "                    f\"transcription started for {sanitized_basename} - estimated run time {estimated_time} mins.txt\"\n",
    "                )\n",
    "                with open(status_file, \"w\") as f:\n",
    "                    f.write(f\"Transcription for {os.path.basename(file_path)} started.\\n\")\n",
    "                    f.write(f\"Estimated time: {estimated_time} mins.\\n\")\n",
    "                print(f\"Created status file: {status_file}\")\n",
    "\n",
    "                # Transcribe the audio file using Whisper v3 pipeline\n",
    "                result = self.pipeline(preprocessed_file, return_timestamps=True)\n",
    "\n",
    "                # Extract and clean the transcription text\n",
    "                transcription_text = nettoyer_texte(result['text'], self.corrections)\n",
    "\n",
    "                # Save the transcription\n",
    "                transcript_file = os.path.join(self.output_dir, os.path.basename(file_path) + '.txt')\n",
    "                with open(transcript_file, \"w\") as f:\n",
    "                    f.write(transcription_text)\n",
    "                print(f\"Transcription saved to: {transcript_file}\")\n",
    "\n",
    "                # Move the processed file to another directory\n",
    "                shutil.move(file_path, os.path.join(self.processed_dir, os.path.basename(file_path)))\n",
    "\n",
    "                # Remove converted file if applicable\n",
    "                if converted:\n",
    "                    os.remove(preprocessed_file)\n",
    "\n",
    "                # Remove the status file\n",
    "                os.remove(status_file)\n",
    "                print(f\"Deleted status file: {status_file}\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError during transcription for {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories for monitoring, output, and processed files\n",
    "    watch_dir = \"../audio_files_drop\"\n",
    "    output_dir = \"../transcripts\"\n",
    "    processed_dir = \"../processed_audios\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "    # Device and model setup\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "    \"\"\"model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "    )\"\"\"\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Load corrections from Excel file\n",
    "    corrections_file = \"../scripts/whisper_errors.xlsx\"\n",
    "    corrections = charger_corrections(corrections_file)\n",
    "\n",
    "    # Set up file observer\n",
    "    observer = Observer()\n",
    "    event_handler = AudioFileHandler(pipe, corrections, output_dir, processed_dir)\n",
    "    observer.schedule(event_handler, path=watch_dir, recursive=False)\n",
    "\n",
    "    observer.start()\n",
    "    print(f\"Watching directory: {watch_dir}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
